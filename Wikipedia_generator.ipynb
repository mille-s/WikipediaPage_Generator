{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BhlvYxvjyG-Z",
        "NMI7l59Z84B3",
        "Jn-kiZwI9h1X",
        "pB1FkV2V5alI",
        "8Hs0EEb-5ppQ",
        "Zwt_QCsb5yvk",
        "3A-5vnL46Dy0"
      ],
      "authorship_tag": "ABX9TyOL0yjbM2s/H3of7Av4X8+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/WikipediaPage_Generator/blob/main/Wikipedia_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Prepare repo"
      ],
      "metadata": {
        "id": "BhlvYxvjyG-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to download and unzip the working folder and install Java 8\n",
        "\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# clone main repo\n",
        "! git clone https://github.com/mille-s/DCU_TCD-FORGe_WebNLG23.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'DCU_TCD-FORGe_WebNLG23/DCU_TCD_FORGe_WebNLG23.ipynb'\n",
        "\n",
        "# clone M-FleNS repo (generation pipeline)\n",
        "! git clone https://github.com/mille-s/M-FleNS_NLG-Pipeline.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'M-FleNS_NLG-Pipeline/M_FleNS_pipe_v2.ipynb'\n",
        "\n",
        "# Download FORGe\n",
        "# Version used for WebNLG (fails to generate a few structures of the training data)\n",
        "# ! gdown 1lsh8pwUp9mc0Z_aFbSy1WTIpSx9YwFFD\n",
        "# ! unzip /content/FORGe_colab_v3_WebNLG.zip\n",
        "# Version used for Mod-D2T (minor improvements on WebNLG)\n",
        "! gdown 196w_EtORTkR3idaXDMq0xl3pOtBrGbiE\n",
        "! unzip /content/FORGe_colab_v4.zip\n",
        "\n",
        "# Download triple to predArg conversion\n",
        "triple2predArg = 'triples2predArg'\n",
        "os.makedirs(triple2predArg)\n",
        "! gdown 1q2MXCvWpNMFBhp40Ssylmu2RDsfbJhWb\n",
        "! unzip 'triples2predArg.zip' -d {triple2predArg}\n",
        "! rm 'triples2predArg.zip'\n",
        "\n",
        "# Download Morphology generator\n",
        "! gdown 1vk1utEjeZ_2YO1H20DPDTjVSevgRJNM_\n",
        "morph_folder_name = 'test_irish_morph_gen_v5.0'\n",
        "zip_name = morph_folder_name+'.zip'\n",
        "! unzip {zip_name}\n",
        "\n",
        "morph_input_folder = '/content/'+morph_folder_name+'/Inputs'\n",
        "morph_output_folder = '/content/'+morph_folder_name+'/Outputs'\n",
        "os.makedirs(morph_input_folder)\n",
        "os.makedirs(morph_output_folder)\n",
        "\n",
        "# Make morphology flookup executable\n",
        "! 7z a -sfx {morph_folder_name}'/flookup.exe' {morph_folder_name}'/flookup'\n",
        "! chmod 755 {morph_folder_name}'/flookup'\n",
        "\n",
        "# Download mock Wikipedia headers\n",
        "! gdown 1MV9y3yvBDtVbLXKYAFSkGPjoax2krwad\n",
        "! gdown 13-X_PRejn3pavQ4f4ltAOvZlklluS1QV\n",
        "shutil.move('/content/wikipedia-header.png', triple2predArg)\n",
        "shutil.move('/content/wikipedia-subheader.png', triple2predArg)\n",
        "\n",
        "# Install SPARQLWrapper and download list of properties\n",
        "! pip install SPARQLWrapper\n",
        "! gdown 1zM1ikqqdyEeT-HHYlN6Grj3f7BoQYcs3\n",
        "props_list_path = '/content/DCU_TCD-FORGe_WebNLG23/sorted_properties.txt'\n",
        "shutil.move('/content/sorted_properties.txt', props_list_path)\n",
        "\n",
        "# Clean\n",
        "! rm '/content/FORGe_colab_v3_WebNLG.zip'\n",
        "! rm '/content/FORGe_colab_v4.zip'\n",
        "! rm '/content/test_irish_morph_gen_v5.0.zip'\n",
        "clear_output()\n",
        "print('Working folder ready!\\n--------------\\nInstalling Java 8...\\n')\n",
        "\n",
        "# Switch to Java 1.8 (needed for FORGe to run correctly)\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !java -version       #check java version\n",
        "install_java()\n",
        "\n",
        "# To wrap texts in cells\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "tdV8-G7AX9Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Get DBpedia properties associated with an entity name"
      ],
      "metadata": {
        "id": "OgraqZRd3RWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set parameters\n",
        "############# Type entity #############\n",
        "name = 'Marie Curie'#@param {type:\"string\"}\n",
        "entity_name = ('_').join(name.split(' '))\n",
        "\n",
        "############# Select class #############\n",
        "category = 'Scientist'#@param['Airport', 'Artist', 'Astronaut', 'Athlete', 'Building', 'CelestialBody', 'City', 'ComicsCharacter', 'Company', 'Film', 'Food', 'MeanOfTransportation', 'Monument', 'MusicalWork', 'Politician', 'Scientist', 'SportsTeam', 'University', 'WrittenWork']\n",
        "input_category = category\n",
        "\n",
        "############# Select language #############\n",
        "language = 'EN' #@param['EN', 'ES', 'GA']\n",
        "\n",
        "############# Triple source #############\n",
        "# To select where to get the triples from. Ontology is supposed to be cleaner but have less coverage.\n",
        "triple_source = 'Infobox' #@param['Ontology', 'Infobox']\n",
        "\n",
        "############# Set properties to discard #############\n",
        "ignore_properties = 'width, title'#@param {type:\"string\"}\n",
        "ignore_properties_input = ignore_properties.split(',')\n",
        "ignore_properties_list = []\n",
        "for ignored_property in ignore_properties_input:\n",
        "  ignore_properties_list.append(ignored_property.strip())\n",
        "# print(ignore_properties_list)\n",
        "\n",
        "############# Select module grouping #############\n",
        "# Group consecutive modules for the same system or call each module separately.\n",
        "# Select 'no' to get all intermediate representations, 'yes' if you're only interested in the output.\n",
        "generate_intermediate_representations = 'no' #@param['yes', 'no']\n",
        "group_modules_prm = ''\n",
        "if generate_intermediate_representations == 'yes':\n",
        "  group_modules_prm = 'no'\n",
        "else:\n",
        "  group_modules_prm = 'yes'\n",
        "\n",
        "############# Select dataset split #############\n",
        "split = \"test\" #@param['dev', 'test','train','ukn']\n",
        "\n",
        "print('Parameters set!')\n",
        "# print(entity_name)\n",
        "# print(input_category)\n",
        "# print(ignore_properties_list)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AiNEj7cLylvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSw0hkloXnV4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get DBpedia properties\n",
        "# Query DBpedia for a given entity\n",
        "# From ChatGPT. Prompt: \"Thanks! Now please write a sparql query that can be used in Python to get all the properties related to Olga Bondareva on DBpedia. For example, birthDate, birthPlace, etc.\"\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from xml.dom import minidom\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout\n",
        "\n",
        "# Read file with all covered properties\n",
        "fd = codecs.open(props_list_path, 'r', 'utf-8')\n",
        "lines_properties = fd.readlines()\n",
        "list_properties = []\n",
        "for line_properties in lines_properties:\n",
        "  line_prop_list = line_properties.strip().split('-')\n",
        "  for prop in line_prop_list:\n",
        "    list_properties.append(prop)\n",
        "\n",
        "# print('There are '+str(len(list_properties))+' different property labels.')\n",
        "# print(sorted(list_properties))\n",
        "\n",
        "class Triple:\n",
        "  def __init__(self, prop, subj_value, obj_value):\n",
        "    self.DBprop = prop\n",
        "    self.DBsubj = subj_value\n",
        "    self.DBobj = obj_value\n",
        "\n",
        "def get_triples_seen(results, subj_name):\n",
        "  # Process and print the results\n",
        "  list_triple_objects = []\n",
        "  for result in results:\n",
        "    # property_uri is something like this: http://dbpedia.org/property/deathPlace\n",
        "    property_uri = result[\"property\"][\"value\"]\n",
        "    # value is a string (1937-04-27) or an entity uri (http://dbpedia.org/resource/Saint_Petersburg)\n",
        "    value = result[\"value\"][\"value\"]\n",
        "    # Get the strings for property and object\n",
        "    if len(value) > 0:\n",
        "      url_triples = ''\n",
        "      if triple_source == 'Ontology':\n",
        "        url_triples = 'http://dbpedia.org/ontology/'\n",
        "      elif triple_source == 'Infobox':\n",
        "        url_triples = 'http://dbpedia.org/property/'\n",
        "      if re.search(url_triples, property_uri):\n",
        "        prop_name = property_uri.rsplit('/', 1)[1]\n",
        "        obj_name = value\n",
        "        if re.search('http://', value):\n",
        "          obj_name = value.rsplit('/', 1)[1]\n",
        "        if prop_name in list_properties and not prop_name in ignore_properties_list:\n",
        "          # print(f\"{prop_name}: {obj_name}\")\n",
        "          triple_object = Triple(prop_name, subj_name, obj_name)\n",
        "          list_triple_objects.append(triple_object)\n",
        "  return(list_triple_objects)\n",
        "\n",
        "def get_properties_of_entity(uri):\n",
        "  # Define the DBpedia SPARQL endpoint URL\n",
        "  sparql_endpoint = \"https://dbpedia.org/sparql\"\n",
        "  # Compose the SPARQL query\n",
        "  sparql_query = f\"\"\"\n",
        "  SELECT ?property ?value\n",
        "  WHERE {{\n",
        "    <{uri}> ?property ?value.\n",
        "  }}\n",
        "  \"\"\"\n",
        "  # Create a SPARQLWrapper object and set the query\n",
        "  sparql = SPARQLWrapper(sparql_endpoint)\n",
        "  sparql.setQuery(sparql_query)\n",
        "  # Set the return format to JSON\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  # Execute the query and parse the results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Activate block to show all properties\n",
        "  # for result in results[\"results\"][\"bindings\"]:\n",
        "  #   # property_uri is something like this: http://dbpedia.org/property/deathPlace\n",
        "  #   property_uri = result[\"property\"][\"value\"]\n",
        "  #   # value is a string (1937-04-27) or an entity uri (http://dbpedia.org/resource/Saint_Petersburg)\n",
        "  #   value = result[\"value\"][\"value\"]\n",
        "  #   print(f\"{property_uri}: {value}\")\n",
        "\n",
        "  # Return the list of properties for the entity\n",
        "  return(results[\"results\"][\"bindings\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  selected_uri = \"http://dbpedia.org/resource/\"+entity_name\n",
        "  # selected_uri = \"http://dbpedia.org/resource/Olga_Bondareva\"\n",
        "  subj_name = selected_uri.rsplit('/', 1)[1]\n",
        "  # Get all properties for entity\n",
        "  results = get_properties_of_entity(selected_uri)\n",
        "  # Get properties covered by the generator and their respective objets\n",
        "  list_triple_objects = get_triples_seen(results, subj_name)\n",
        "\n",
        "  # Check\n",
        "  # print('Subject: '+subj_name)\n",
        "  # for triple_object in list_triple_objects:\n",
        "  #   print(triple_object.DBprop+' : '+triple_object.DBobj)\n",
        "\n",
        "  list_prop = []\n",
        "  list_obj = []\n",
        "  list_propObj =[]\n",
        "  for n, triple_object in enumerate(list_triple_objects):\n",
        "    list_prop.append(triple_object.DBprop)\n",
        "    list_obj.append(triple_object.DBobj)\n",
        "    list_propObj.append(str(n)+' - '+triple_object.DBprop+': '+triple_object.DBobj)\n",
        "\n",
        "  # Create a table to visualise properties\n",
        "  # property_names = pd.Series(list_prop)\n",
        "  # object_names = pd.Series(list_obj)\n",
        "  # data_frame = pd.DataFrame({ 'Property': property_names, 'Value': object_names })\n",
        "  # display(data_frame)\n",
        "  # To see table with cells left-aligned\n",
        "  # left_aligned_df = left_aligned_df.set_table_styles([dict(selector = 'th', props=[('text-align', 'left')])])\n",
        "  # display(left_aligned_df)\n",
        "\n",
        "  # Create slider for number of properties\n",
        "  # num_triples = widgets.IntSlider(value=1, max=50)\n",
        "  # display(num_triples)\n",
        "\n",
        "  # Create property selector\n",
        "  selected_poperties = widgets.SelectMultiple(\n",
        "      options=list_propObj,\n",
        "      value=[],\n",
        "      rows=len(list_propObj),\n",
        "      description='Properties',\n",
        "      layout=Layout(width='642px'),\n",
        "      disabled=False\n",
        "  )\n",
        "  display(selected_poperties)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Generate text"
      ],
      "metadata": {
        "id": "NMI7l59Z84B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation parameters"
      ],
      "metadata": {
        "id": "Jn-kiZwI9h1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "\n",
        "# Modules to run, with type of processing (FORGe, Model1, SimpleNLG, etc.).\n",
        "# Only FORGe is supported for this prototype version.\n",
        "PredArg_Normalisation = 'FORGe'\n",
        "# To have an external module assigning triples to aggregate\n",
        "PredArg_AggregationMark = 'None'\n",
        "PredArg_Aggregation = 'FORGe'\n",
        "PredArg_PoSTagging = 'FORGe'\n",
        "PredArg_CommStructuring = 'FORGe'\n",
        "DSynt_Structuring = 'FORGe'\n",
        "SSynt_Structuring = 'FORGe'\n",
        "SSynt_Aggregation = 'FORGe'\n",
        "RE_Generation = 'FORGe'\n",
        "DMorph_AgreementsLinearisation = 'FORGe'\n",
        "SMorph_Processing = 'FORGe'\n",
        "\n",
        "#######################################################################\n",
        "# Paths to python files\n",
        "path_MFleNS = '/content/M-FleNS_NLG-Pipeline/code/M-FleNS.py'\n",
        "path_checkOutputs = '/content/M-FleNS_NLG-Pipeline/code/M-FleNS-checkOutputs.py'\n",
        "path_postProc = '/content/M-FleNS_NLG-Pipeline/code/postProcess.py'\n",
        "path_FORGe2Morph = '/content/DCU_TCD-FORGe_WebNLG23/code/FORGe2Morph.py'\n",
        "path_concatenate = '/content/M-FleNS_NLG-Pipeline/code/concatenate_files.py'\n",
        "# path_MorphGen = '/content/DCU_TCD-FORGe_WebNLG23/code/IrishNLP_MorphGen.py'\n",
        "\n",
        "#######################################################################\n",
        "# Paths to FORGe/MATE folders and property files\n",
        "FORGe_input_folder = '/content/FORGe/buddy_project/struct'\n",
        "path_MATE = '/content/FORGe/buddy-patched.jar'\n",
        "path_props_resources_template = '/content/FORGe/mateColabDrive.properties'\n",
        "path_props_levels = '/content/FORGe/mateLevels.properties'\n",
        "path_props = '/content/FORGe/mate.properties'\n",
        "\n",
        "# Paths to general folders\n",
        "# The input structure(s) of the correct type should be placed in the folder that corresponds to the first module called in the next cell\n",
        "path_strs = '/content/FORGe/structures'\n",
        "str_PredArg_folder = os.path.join(path_strs, '00-PredArg')\n",
        "str_PredArgNorm_folder = os.path.join(path_strs, '01-PredArgNorm')\n",
        "str_PredArgAggMark_folder = os.path.join(path_strs, '02-PredArgAggMark')\n",
        "str_PredArgAgg_folder = os.path.join(path_strs, '03-PredArgAgg')\n",
        "str_PredArgPoS_folder = os.path.join(path_strs, '04-PredArgPoS')\n",
        "str_PredArgComm_folder = os.path.join(path_strs, '05-PredArgComm')\n",
        "str_DSynt_folder = os.path.join(path_strs, '06-DSynt')\n",
        "str_SSynt_folder = os.path.join(path_strs, '07-SSynt')\n",
        "str_SSyntAgg_folder = os.path.join(path_strs, '08-SSyntAgg')\n",
        "str_REG_folder = os.path.join(path_strs, '09-REG')\n",
        "str_DMorphLin_folder = os.path.join(path_strs, '10-DMorphLin')\n",
        "str_SMorphText_folder = os.path.join(path_strs, '11-SMorphText')\n",
        "log_folder = '/content/FORGe/log'\n",
        "if not os.path.exists(log_folder):\n",
        "  os.makedirs(log_folder)\n",
        "temp_input_folder_morph = '/content/FORGe-out'\n",
        "if not os.path.exists(temp_input_folder_morph):\n",
        "  os.makedirs(temp_input_folder_morph)\n",
        "\n",
        "def clear_files(folder):\n",
        "  \"Function to clear files from a folder.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    for filename in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, filename)\n",
        "      try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "          os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "          shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "# empty FORGe input folder\n",
        "clear_files(str_PredArg_folder)"
      ],
      "metadata": {
        "id": "vIntos-p9Y-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.1: Convert DBpedia triples to linguistic structures"
      ],
      "metadata": {
        "id": "pB1FkV2V5alI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate list of indices of properties selected by user (index in the list of Triple objects that contains all retrieved triples)\n",
        "properties_selected_by_user = []\n",
        "\n",
        "if len(selected_poperties.value) == 0:\n",
        "  x = 0\n",
        "  while x < len(list_triple_objects):\n",
        "    properties_selected_by_user.append(x)\n",
        "    x += 1\n",
        "else:\n",
        "  for selected_property in selected_poperties.value:\n",
        "    properties_selected_by_user.append(int(selected_property.split(' - ')[0]))\n",
        "\n",
        "print(properties_selected_by_user)"
      ],
      "metadata": {
        "id": "oGeXtgqxoSPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert retrieved triples to XML\n",
        "\n",
        "def create_xml(triple_objects, properties_selected_by_user, triple2predArgPath):\n",
        "  \"\"\" Create the XML file with the triples to be converted to PredArg\"\"\"\n",
        "  n = len(properties_selected_by_user)\n",
        "  # Create nodes\n",
        "  root = minidom.Document()\n",
        "  xml = root.createElement('benchmark')\n",
        "  entries = root.createElement('entries')\n",
        "  entry = root.createElement('entry')\n",
        "  original_ts = root.createElement('originaltripleset')\n",
        "  modified_ts = root.createElement('modifiedtripleset')\n",
        "  lex = root.createElement('lex')\n",
        "  # Create structure between nodes\n",
        "  root.appendChild(xml)\n",
        "  xml.appendChild(entries)\n",
        "  entries.appendChild(entry)\n",
        "  entry.appendChild(original_ts)\n",
        "  entry.appendChild(modified_ts)\n",
        "  entry.appendChild(lex)\n",
        "  # Create main attributes\n",
        "  entry.setAttribute('category', str(input_category))\n",
        "  entry.setAttribute('eid', '1')\n",
        "  entry.setAttribute('shape', '(X (X) (X) (X) (X))')\n",
        "  entry.setAttribute('shape-type', 'sibling')\n",
        "  entry.setAttribute('size', str(n))\n",
        "  # Create lex attributes\n",
        "  lex.setAttribute('comment', '')\n",
        "  lex.setAttribute('lid', 'id1')\n",
        "  lex.setAttribute('lang', 'ga')\n",
        "  fake_text = root.createTextNode('Some Irish text.')\n",
        "  lex.appendChild(fake_text)\n",
        "  # Fill in otriple and mtriple fields with the same info\n",
        "  x = 0\n",
        "  while x < len(triple_objects):\n",
        "  # for triple_object in triple_objects:\n",
        "    triple_object = triple_objects[x]\n",
        "    if x in properties_selected_by_user:\n",
        "      text1 = root.createTextNode(triple_object.DBsubj+' | '+triple_object.DBprop+' | '+triple_object.DBobj)\n",
        "      otriple = root.createElement('otriple')\n",
        "      original_ts.appendChild(otriple)\n",
        "      otriple.appendChild(text1)\n",
        "      text2 = root.createTextNode(triple_object.DBsubj+' | '+triple_object.DBprop+' | '+triple_object.DBobj)\n",
        "      mtriple = root.createElement('mtriple')\n",
        "      modified_ts.appendChild(mtriple)\n",
        "      mtriple.appendChild(text2)\n",
        "    x += 1\n",
        "\n",
        "  xml_str = root.toprettyxml(indent =\"  \")\n",
        "  save_path_file = os.path.join(triple2predArgPath, str(triple_object.DBsubj)+\".xml\")\n",
        "\n",
        "  with open(save_path_file, \"w\") as f:\n",
        "      f.write(xml_str)\n",
        "\n",
        "create_xml(list_triple_objects, properties_selected_by_user, triple2predArg)"
      ],
      "metadata": {
        "id": "piiWre5Bp8Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and write class and gender information\n",
        "\n",
        "def update_file_class_members(filepath, list_members):\n",
        "  fd = codecs.open(filepath, 'r', 'utf-8').readlines()\n",
        "  # We build this list and write it all to be safer (sometimes the all_validated files end with an empty line, sometimes not)\n",
        "  list_members_to_write = []\n",
        "  for member_there in fd:\n",
        "    list_members_to_write.append(member_there.strip())\n",
        "\n",
        "  for member_to_add in list_members:\n",
        "    if member_to_add not in list_members_to_write:\n",
        "      list_members_to_write.append(member_to_add)\n",
        "\n",
        "  with codecs.open(filepath, 'w', 'utf-8') as fo:\n",
        "    for member_to_write in list_members_to_write:\n",
        "      fo.write(member_to_write)\n",
        "      fo.write('\\n')\n",
        "\n",
        "def get_types_of_entity(entity_untyped):\n",
        "  list_types_found = []\n",
        "\n",
        "  uri = \"http://dbpedia.org/resource/\"+entity_untyped\n",
        "  # Define the DBpedia SPARQL endpoint URL\n",
        "  sparql_endpoint = \"https://dbpedia.org/sparql\"\n",
        "  # Compose the SPARQL query\n",
        "  sparql_query = f\"\"\"\n",
        "  SELECT ?obj\n",
        "  WHERE {{\n",
        "    <{uri}> rdf:type ?obj.\n",
        "  }}\n",
        "  \"\"\"\n",
        "  # Create a SPARQLWrapper object and set the query\n",
        "  sparql = SPARQLWrapper(sparql_endpoint)\n",
        "  sparql.setQuery(sparql_query)\n",
        "  # Set the return format to JSON\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  # Execute the query and parse the results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Get types\n",
        "  for result in results[\"results\"][\"bindings\"]:\n",
        "    if re.search('yago/Woman', result[\"obj\"][\"value\"]) and 'Woman' not in list_types_found:\n",
        "      list_types_found.append('Woman')\n",
        "    type_value = result[\"obj\"][\"value\"].rsplit('/', 1)[1]\n",
        "    if type_value == 'Person':\n",
        "      # to filter out entities that are probably not persons (e.g. \"Politician\")\n",
        "      if re.search('_', entity_untyped):\n",
        "        if type_value not in list_types_found:\n",
        "          list_types_found.append(type_value)\n",
        "    if type_value == 'Band':\n",
        "      if type_value not in list_types_found:\n",
        "        list_types_found.append(type_value)\n",
        "  return(list_types_found)\n",
        "\n",
        "  # return(results[\"results\"][\"bindings\"])\n",
        "\n",
        "band_subj = []\n",
        "band_obj = []\n",
        "female_subj = []\n",
        "female_obj = []\n",
        "person_subj = []\n",
        "person_obj = []\n",
        "\n",
        "# Create a list with all entities that need a type\n",
        "obj_that_need_type = []\n",
        "for obj_name in list_obj:\n",
        "  if not re.search(' ', obj_name) and not obj_name.isnumeric() and obj_name[0].isupper() and obj_name not in obj_that_need_type:\n",
        "    obj_that_need_type.append(obj_name)\n",
        "\n",
        "# For each of the entities that need a type, get the types\n",
        "for obj_entity_untyped in obj_that_need_type:\n",
        "  results_type_obj = get_types_of_entity(obj_entity_untyped)\n",
        "  obj_entity_clean = obj_entity_untyped.rsplit('_(', 1)[0]\n",
        "  x = 0\n",
        "  while x < len(results_type_obj):\n",
        "    result_type = results_type_obj[x]\n",
        "    if result_type == 'Woman':\n",
        "      female_obj.append(obj_entity_clean)\n",
        "    if result_type == 'Person':\n",
        "      person_obj.append(obj_entity_clean)\n",
        "    if result_type == 'Band':\n",
        "      band_obj.append(obj_entity_clean)\n",
        "    x += 1\n",
        "\n",
        "results_type_subj = get_types_of_entity(entity_name)\n",
        "y = 0\n",
        "while y < len(results_type_subj):\n",
        "  result_type = results_type_subj[y]\n",
        "  if result_type == 'Woman':\n",
        "    female_subj.append(entity_name)\n",
        "  if result_type == 'Person':\n",
        "    person_subj.append(entity_name)\n",
        "  if result_type == 'Band':\n",
        "    band_subj.append(entity_name)\n",
        "  y += 1\n",
        "\n",
        "update_file_class_members('/content/triples2predArg/classMembership/band_obj_all_validated.txt', band_obj)\n",
        "update_file_class_members('/content/triples2predArg/classMembership/band_sub_all_validated.txt', band_subj)\n",
        "update_file_class_members('/content/triples2predArg/classMembership/female_obj_all_validated.txt', female_obj)\n",
        "update_file_class_members('/content/triples2predArg/classMembership/female_sub_all_validated.txt', female_subj)\n",
        "update_file_class_members('/content/triples2predArg/classMembership/person_obj_all_validated.txt', person_obj)\n",
        "update_file_class_members('/content/triples2predArg/classMembership/person_sub_all_validated.txt', person_subj)\n",
        "\n",
        "print('Band-sbj: '+str(band_subj))\n",
        "print('Band-obj: '+str(band_obj))\n",
        "print('Fem-sbj: '+str(female_subj))\n",
        "print('Fem-obj: '+str(female_obj))\n",
        "print('Per-sbj: '+str(person_subj))\n",
        "print('Per-obj: '+str(person_obj))"
      ],
      "metadata": {
        "id": "Wm6JmQB-J98P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_t2p = language.lower()\n",
        "path_t2p_out = os.path.join(triple2predArg, 'out/')\n",
        "clear_files(path_t2p_out)\n",
        "\n",
        "name_conll_templates = ''\n",
        "\n",
        "if language == 'GA':\n",
        "  name_conll_templates = '221130_WebNLG23_GA.conll'\n",
        "else:\n",
        "  name_conll_templates = '230528-WebNLG23_EN.conll'\n",
        "\n",
        "# Convert xml into predArg\n",
        "!java -jar '/content/triples2predArg/webNLG_triples2conll.jar' '/content/triples2predArg/' {name_conll_templates} '230528-WebNLG23_EN-GA_properties.txt' {path_t2p_out} {language_t2p} {entity_name}  # -> \"log.txt\"\n",
        "\n",
        "# Copy conll file to FORGe input folder\n",
        "shutil.copy(os.path.join(path_t2p_out, entity_name+'_'+language_t2p+'.conll'), str_PredArg_folder)"
      ],
      "metadata": {
        "id": "xGu3-Vj75gQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty input folder to copy other inputs instead\n",
        "# list_predArgPathsCC = glob.glob(os.path.join('/content/FORGe/structures/00-PredArg/', '*.conll'))\n",
        "# c = 0\n",
        "# for predArgPathCC in list_predArgPathsCC:\n",
        "#   ! rm {predArgPathCC}\n",
        "#   c += 1\n",
        "# print('Removed '+str(c)+' files.')"
      ],
      "metadata": {
        "id": "Dg5pgV0F5mPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2: Convert linguistic structures into non-inflected Irish text (FORGe generator)"
      ],
      "metadata": {
        "id": "8Hs0EEb-5ppQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch generation process\n",
        "! python {path_MFleNS} {language} {split} {group_modules_prm} {PredArg_Normalisation} {PredArg_AggregationMark} {PredArg_Aggregation} {PredArg_PoSTagging} {PredArg_CommStructuring} {DSynt_Structuring} {SSynt_Structuring} {SSynt_Aggregation} {RE_Generation} {DMorph_AgreementsLinearisation} {SMorph_Processing} {FORGe_input_folder} {path_MATE} {path_props_resources_template} {path_props_levels} {path_props} {str_PredArg_folder} {str_PredArgNorm_folder} {str_PredArgAggMark_folder} {str_PredArgAgg_folder} {str_PredArgPoS_folder} {str_PredArgComm_folder} {str_DSynt_folder} {str_SSynt_folder} {str_SSyntAgg_folder} {str_REG_folder} {str_DMorphLin_folder} {str_SMorphText_folder} {log_folder}\n"
      ],
      "metadata": {
        "id": "fSje-IhV5pH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check outputs and copy files to morph folder\n",
        "import shutil\n",
        "\n",
        "def clear_folder(folder):\n",
        "  \"Function to clear whole folders.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    try:\n",
        "      shutil.rmtree(folder)\n",
        "    except Exception as e:\n",
        "      print('Failed to delete %s. Reason: %s' % (folder, e))\n",
        "\n",
        "# # Read original check script\n",
        "# check_script = open(path_checkOutputs, 'r')\n",
        "# lines_check_script = check_script.readlines()\n",
        "# # Update check script\n",
        "# with codecs.open(path_checkOutputs, 'w', 'utf-8') as f:\n",
        "#   for line in lines_check_script:\n",
        "#     if line.startswith('log_folder = sys.argv[3]'):\n",
        "#       f.write('log_folder = sys.argv[3]\\ntemp_input_folder_morph = sys.argv[4]\\nlanguage = sys.argv[5]\\n')\n",
        "#     elif line.startswith('            count_perLevel.append(count)\\n'):\n",
        "#       f.write('            count_perLevel.append(count)\\n            if language == \"GA\":\\n              shutil.copy(new_file_path, temp_input_folder_morph)\\n')\n",
        "#     else:\n",
        "#       f.write(line)\n",
        "\n",
        "! python {path_checkOutputs} {str_PredArg_folder} {str_SMorphText_folder} {log_folder} {temp_input_folder_morph} {language}\n",
        "\n",
        "if not language == 'GA':\n",
        "  clear_folder(os.path.join(temp_input_folder_morph, split))\n",
        "  # For GA, files are copied from the python code called above\n",
        "  ! python {path_concatenate} {str_SMorphText_folder} {temp_input_folder_morph} {split}"
      ],
      "metadata": {
        "id": "a_AUx4SF5w4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.3: Inflect Irish text (Irish NLP tools)"
      ],
      "metadata": {
        "id": "Zwt_QCsb5yvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process raw FORGe output and format it for Morphology\n",
        "\n",
        "FORGe_log = open('/content/FORGe/log/summary.txt', 'r')\n",
        "lines_log = FORGe_log.readlines()\n",
        "# Get number of expected texts\n",
        "count_strs_all_FORGe = 0\n",
        "for line in lines_log:\n",
        "  if line.startswith('Outputs: '):\n",
        "    count_strs_all_FORGe = int(line.strip().split('Outputs: ')[-1])\n",
        "\n",
        "print('Expected texts: '+str(count_strs_all_FORGe)+'.\\n')\n",
        "\n",
        "if language == 'GA':\n",
        "  ! python {path_FORGe2Morph} {language} {temp_input_folder_morph} {morph_input_folder}\n",
        "  clear_files(temp_input_folder_morph)"
      ],
      "metadata": {
        "id": "lQkLf0cJ582k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call morph generator\n",
        "# v3 (fast, ~2sec/450 texts)\n",
        "\n",
        "# Run the morphology generation\n",
        "from IPython.display import HTML, display\n",
        "import progressbar\n",
        "import glob\n",
        "import codecs\n",
        "from termcolor import colored\n",
        "import re\n",
        "\n",
        "show_input = False #@param {type:\"boolean\"}\n",
        "\n",
        "if language == 'GA':\n",
        "  clear_files(morph_output_folder)\n",
        "  # To store how many texts we have in each file (used to )\n",
        "  count_strs_all_Morph = []\n",
        "  for filepath in sorted(glob.glob(os.path.join(morph_input_folder, '*.*'))):\n",
        "    count_strs_all = 0\n",
        "    head, tail = os.path.split(filepath)\n",
        "    filename = tail.rsplit('.')[0]\n",
        "    print('Processing '+filename)\n",
        "    fo = codecs.open(morph_output_folder+'/'+filename+'_out.txt', 'w', 'utf-8')\n",
        "    list_inflected_words = ! cat {filepath} | {morph_folder_name}'/flookup' -a {morph_folder_name}'/allgen.fst'\n",
        "    # print(list_inflected_words)\n",
        "\n",
        "    # Create a variable to store the outputs\n",
        "    text = ''\n",
        "    # morph returns this as list_inflected_words: ['imir+Verb+Vow+PresInd\\timríonn', '', 'Agremiação_Sportiva_Arapiraquense+Noun+Masc+Com+Pl\\t+?', '', ',\\t+?',...]\n",
        "    for word in list_inflected_words:\n",
        "      empty = 'yes'\n",
        "      input_string = ''\n",
        "      morph_returned = ''\n",
        "      morph_backup = ''\n",
        "      if re.search('\\t', word):\n",
        "        # for every space an empty string is returned; we'll ignore them later. Between two consecutive texts there is a simple \"\\t\" with nothing around. I use this to introduce linebreaks later.\n",
        "        empty = 'no'\n",
        "        input_string = word.split('\\t')[0]\n",
        "        morph_returned = word.split('\\t')[1]\n",
        "        if re.search('\\+', word):\n",
        "          morph_backup = input_string.split('+', 1)[0]\n",
        "        else:\n",
        "          morph_backup = input_string\n",
        "      out_line = ''\n",
        "      # Create each output line with the required contents\n",
        "      if show_input == True:\n",
        "        if empty == 'no':\n",
        "          if morph_returned == '':\n",
        "            if input_string == '':\n",
        "              out_line = out_line + '\\n'\n",
        "              count_strs_all += 1\n",
        "          else:\n",
        "            out_line = out_line + input_string + ': ' +'\\x1b[5;30;47m'+morph_returned+'\\x1b[0m'+'\\n'\n",
        "      else:\n",
        "        if empty == 'no':\n",
        "          if morph_returned == '+?':\n",
        "            out_line = out_line + morph_backup + ' '\n",
        "          # If the line is empty, add a line break (empty lines separate different texts in the input)\n",
        "          elif morph_returned == '':\n",
        "            if input_string == '':\n",
        "              out_line = out_line + '\\n'\n",
        "              count_strs_all += 1\n",
        "          else:\n",
        "            out_line = out_line + morph_returned + ' '\n",
        "      # add line to the other lines of the same file\n",
        "      text = text + out_line\n",
        "\n",
        "    # print('\\n----------------------\\n'+text+'\\n')\n",
        "    count_strs_all_Morph.append(count_strs_all)\n",
        "    fo.write(text+'\\n')\n",
        "    fo.close()\n",
        "\n",
        "  # Check\n",
        "  with codecs.open('/content/FORGe/log/summary.txt', 'a', 'utf-8') as fo:\n",
        "    fo.write('\\nMorphology debug\\n==================\\n\\n')\n",
        "    if not sum(count_strs_all_Morph) == count_strs_all_FORGe:\n",
        "      print('\\nERROR! Mismatch with FORGe outputs!')\n",
        "      fo.write('ERROR! Mismatch with FORGe outputs!\\n')\n",
        "    print('\\nThere are '+str(sum(count_strs_all_Morph))+' texts.')\n",
        "    fo.write('There are '+str(sum(count_strs_all_Morph))+' texts.\\n')\n",
        "    print('Texts per file: '+str(count_strs_all_Morph))\n",
        "    fo.write('Texts per file: '+str(count_strs_all_Morph)+'\\n')\n",
        "    fo.write('---------------------------------\\n')\n",
        "  clear_files(morph_input_folder)"
      ],
      "metadata": {
        "id": "_BROPwE05-iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.4: Post-process output"
      ],
      "metadata": {
        "id": "3A-5vnL46Dy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process texts\n",
        "prefinal_output_folder = ''\n",
        "\n",
        "if language == 'GA':\n",
        "  prefinal_output_folder = morph_output_folder\n",
        "else:\n",
        "  prefinal_output_folder = os.path.join(temp_input_folder_morph, split)\n",
        "\n",
        "! python {path_postProc} {language} {prefinal_output_folder}\n",
        "\n",
        "# Check\n",
        "list_filepaths = glob.glob(os.path.join(prefinal_output_folder, '*_postproc.txt'))\n",
        "count_strs_all_postproc = []\n",
        "for filepath in sorted(list_filepaths):\n",
        "  count_strs_all = 0\n",
        "  head, tail = os.path.split(filepath)\n",
        "  fd = codecs.open(filepath, 'r', 'utf-8')\n",
        "  lines = fd.readlines()\n",
        "  x = 0\n",
        "  for line in lines:\n",
        "    if not line == '\\n':\n",
        "      count_strs_all += 1\n",
        "    x += 1\n",
        "  count_strs_all_postproc.append(count_strs_all)\n",
        "\n",
        "with codecs.open('/content/FORGe/log/summary.txt', 'a', 'utf-8') as fo:\n",
        "  fo.write('\\nPost-processing debug\\n==================\\n\\n')\n",
        "  if not sum(count_strs_all_postproc) == count_strs_all_FORGe:\n",
        "    print('\\nERROR! Mismatch with FORGe outputs!')\n",
        "    fo.write('ERROR! Mismatch with FORGe outputs!\\n')\n",
        "  print('\\nThere are '+str(sum(count_strs_all_postproc))+' texts.')\n",
        "  fo.write('There are '+str(sum(count_strs_all_postproc))+' texts.\\n')\n",
        "  print('Texts per file: '+str(count_strs_all_postproc))\n",
        "  fo.write('Texts per file: '+str(count_strs_all_postproc)+'\\n')\n",
        "  fo.write('---------------------------------\\n')"
      ],
      "metadata": {
        "id": "SmpUvibw6H2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate files\n",
        "\n",
        "list_clean_outputs = ''\n",
        "if language == 'GA':\n",
        "  list_clean_outputs = glob.glob(os.path.join(morph_output_folder, '*_out_postproc.txt'))\n",
        "else:\n",
        "  list_clean_outputs = glob.glob(os.path.join(temp_input_folder_morph, split, '*_postproc.txt'))\n",
        "print(list_clean_outputs)\n",
        "\n",
        "filename = 'all_'+language+'_'+split+'_out.txt'\n",
        "\n",
        "with codecs.open(filename, 'w', 'utf-8') as outfile:\n",
        "  # Files need to be sorted to be concatenated in the right order\n",
        "  for fname in sorted(list_clean_outputs):\n",
        "    print('Processing '+fname)\n",
        "    with open(fname) as infile:\n",
        "      outfile.write(infile.read())\n",
        "\n",
        "# Check\n",
        "with codecs.open('/content/FORGe/log/summary.txt', 'a', 'utf-8') as fo:\n",
        "  fo.write('\\nConcatenate debug\\n==================\\n\\n')\n",
        "  count_texts_all = len(codecs.open(filename).readlines())\n",
        "  if not count_texts_all == count_strs_all_FORGe:\n",
        "    print('\\nERROR! Mismatch with FORGe outputs!')\n",
        "    fo.write(('ERROR! Mismatch with FORGe outputs!\\n'))\n",
        "  print('\\nThere are '+str(count_texts_all)+' texts.')\n",
        "  fo.write('There are '+str(count_texts_all)+' texts.\\n')\n"
      ],
      "metadata": {
        "id": "h_fZLFg06Jnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip and download FORGE output folder to download intermediate representations\n",
        "# from google.colab import files\n",
        "# zip_name_inter = '/content/WebNLG_['+language+']_['+split+']_allLevels.zip'\n",
        "# !zip -r {zip_name_inter} /content/FORGe/structures\n",
        "\n",
        "# clear_output()\n",
        "\n",
        "# files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "-Ayh52376LLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip FORGe log files folder\n",
        "# from google.colab import files\n",
        "# zip_name_log = '/content/WebNLG_['+language+']_['+split+']_logs.zip'\n",
        "# !zip -r {zip_name_log} /content/FORGe/log\n",
        "\n",
        "# clear_output()\n",
        "\n",
        "# files.download(zip_name_log)"
      ],
      "metadata": {
        "id": "FewbcFh96MOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Show text - You can download the *_out.txt file at the bottom on the leftside"
      ],
      "metadata": {
        "id": "G0Fj4lWGxOLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "display(Image('/content/triples2predArg/wikipedia-header.png', width=1400, height=92))\n",
        "title = '#'+name+''\n",
        "display(Markdown(title))\n",
        "display(Image('/content/triples2predArg/wikipedia-subheader.png', width=1110, height=88))\n",
        "\n",
        "with codecs.open(filename, 'r', 'utf-8') as text:\n",
        "  for line in text:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "-y2WAncs3NjO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
